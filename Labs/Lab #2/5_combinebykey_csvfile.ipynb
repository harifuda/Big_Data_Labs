{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eada6c52-c201-4bd5-9665-e5e9541a859d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Managing Big Data for Connected Devices\n",
    "\n",
    "## 420-N63-NA\n",
    "\n",
    "## Kawser Wazed Nafi\n",
    "\n",
    "--------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc06e02d-a35d-4d7e-9939-e0827262b6b8",
   "metadata": {},
   "source": [
    "## Foreach Accumulator Combineby CSVfile Operation\n",
    "\n",
    "\n",
    "### Foreach\n",
    "PySpark foreach() is an action operation that is available in RDD, DataFram to iterate/loop over each element in the DataFrmae, It is similar to for with advanced concepts\n",
    "\n",
    "### Syntax\n",
    "foreach(lambda function)\n",
    "\n",
    "### Example 1\n",
    "for a given dataframe, [1,2,3,4,5], we want to add 5 with each of the values in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7742c02-b6a3-4267-9882-9184bee8bb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "sparksession = SparkSession.builder.master(\"local[4]\") \\\n",
    "                    .appName('accumulatorAPP') \\\n",
    "                    .getOrCreate()\n",
    "sc = sparksession.sparkContext\n",
    "\n",
    "rdd = sc.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x: print(x+5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01329d9-21db-4d7c-a7c1-95cb38e025ce",
   "metadata": {},
   "source": [
    "You might not get anything printed over here. A reason for this is the printing method is running of cluster. Each the values are accessing one by one.\n",
    "\n",
    "### Accumulator\n",
    "\n",
    "The PySpark Accumulator is a shared variable that is used with RDD and DataFrame to perform sum and counter operations similar to Map-reduce counters. These variables are shared by all executors to update and add information through aggregation or computative operations.\n",
    "\n",
    "Accumulators are write-only and initialize once variables where only tasks that are running on workers are allowed to update and updates from the workers get propagated automatically to the driver program. But, only the driver program is allowed to access the Accumulator variable using the value property.\n",
    "\n",
    "<ul>\n",
    "<li><strong>sparkContext.accumulator()</strong> is used to define accumulator variables.</li>\n",
    "<li><strong>add()</strong> function is used to add/update a value in accumulator</li>\n",
    "<li><strong>value</strong> property on the accumulator variable is used to retrieve the value from the accumulator.</li>\n",
    "</ul>\n",
    "\n",
    "### Example 2\n",
    "\n",
    "In this following program, the RDD is going to used by accumulator where all the values of the RDD are going to be added together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "022c3ee4-d318-44f6-ad4e-f604d9ad1413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "accum=sc.accumulator(0)\n",
    "rdd=sc.parallelize([1,2,3,4,5])\n",
    "rdd.foreach(lambda x:accum.add(x))\n",
    "print(accum.value) #Accessed by driver\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f597c3-ed70-434d-8499-8ad2f841eed0",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
    "Can you explain the difference between the PySpark Map() and PySpark Foreach()? Search for it and state over here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45e3d9-dd9b-4497-9526-cea689cfc159",
   "metadata": {},
   "source": [
    ".map() is used to apply a transformation lambda/function on each element in an RDD and returns a new RDD.\n",
    "\n",
    ".forEach() is used to iterate over each element in an RDD, it does not return anything."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b289f12-6638-4ae5-8dd3-961869c23719",
   "metadata": {},
   "source": [
    "### CombineByKey\n",
    "Generic function to combine the elements for each key using a custom set of aggregation functions. It works as association rule.\n",
    "\n",
    "Users provide three functions:\n",
    "<ul>\n",
    "<li>createCombiner, which turns a V into a C (e.g., creates a one-element list)</li>\n",
    "\n",
    "<li>mergeValue, to merge a V into a C (e.g., adds it to the end of a list)</li>\n",
    "\n",
    "<li>mergeCombiners, to combine two Câ€™s into a single one (e.g., merges the lists)</li>\n",
    "</ul>\n",
    "To avoid memory allocation, both mergeValue and mergeCombiners are allowed to modify and return their first argument instead of creating a new C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9898ad3-d697-480b-8d84-198174befce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', [1, 2]), ('b', [1])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 2)])\n",
    "def createcombiner(a):\n",
    "    return [a]\n",
    "\n",
    "def mergerValue(a, b):\n",
    "    a.append(b)\n",
    "    return a\n",
    "\n",
    "def combiners(a, b):\n",
    "    a.extend(b)\n",
    "    return a\n",
    "\n",
    "sorted(x.combineByKey(createcombiner, mergerValue, combiners).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35e0449-d314-4cf7-a473-01b7442534b3",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 2\n",
    "\n",
    "The following data/list is given to you: (\"a\", 1), (\"b\", 1), (\"c\", 2), (\"a\", 3), (\"b\", 5), (\"c\", 5), (\"c\", 4), (\"a\", 2), (\"b\", 2). Use the CombineByKey\n",
    "in such a way that it will result in the following combination : [('a', [6]), ('b', [8]), ('c', [11])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "457bbf3b-4fa9-4bb0-9f21-e044bbad8084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 6), ('b', 8), ('c', 11)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"c\", 2), (\"a\", 3), (\"b\", 5), (\"c\", 5), (\"c\", 4), (\"a\", 2), (\"b\", 2)])\n",
    "\n",
    "def add(x, y):\n",
    "    return int(x) + int(y)\n",
    "\n",
    "sorted(l.combineByKey(str, add, add).collect())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01651b49-c78e-4632-83b9-41dd518e976e",
   "metadata": {},
   "source": [
    "### CSVfile with RDD\n",
    "\n",
    "CSVfile is open of the popular way to store the big data in a categorized way. In our last class, we have seen how to read a csv file into rdd. In this assignment, we will perform operations with csvfile, filter data from the given csvfile and analysis the data\n",
    "\n",
    "To work on that direction, we will perform today's lab operation of the following dataset: MovieLens Dataset.\n",
    "\n",
    "To get the dataset go to https://grouplens.org/datasets/movielens/ \n",
    "\n",
    "and download the ml-latest-small.zip \n",
    "\n",
    "you will find it under \"recommended for education and development\" which has 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users.\n",
    "\n",
    "Unzip the folder and add the folder to the directory mapped to your container. It will be visible in the left option section. You can also upload all the files using the upload button in the jupyter notebook.\n",
    "\n",
    "RDD is unstructured data storage. So whether it is a CSV file separated by a comma or not, we will be just loading it as a text file and we will get each line as an entry in the RDD.\n",
    "\n",
    "In addition, we are going to use take() with RDD. Take merges all the partitons and pull the number of the rows mentioned inside take method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e8d782d-84a8-4049-befb-412ad3395bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId,movieId,rating,timestamp',\n",
       " '1,1,4.0,964982703',\n",
       " '1,3,4.0,964981247',\n",
       " '1,6,4.0,964982224',\n",
       " '1,47,5.0,964983815']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsRDDWithHeader = sc.textFile('ratings.csv')\n",
    "ratingsRDDWithHeader.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e376a4-1e38-49b6-8c3b-8125719c1be1",
   "metadata": {},
   "source": [
    "From the Data, we can see that, the file has a header. We can see that by using the `first` function on the RDD which is an action function that will return the first item in the RDD. After loading the data to\n",
    "RDD, we can filter that row just to get the rest of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0aebe68-a1ec-4859-ab9f-91260c0626f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "userId,movieId,rating,timestamp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['1,1,4.0,964982703',\n",
       " '1,3,4.0,964981247',\n",
       " '1,6,4.0,964982224',\n",
       " '1,47,5.0,964983815',\n",
       " '1,50,5.0,964982931']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = ratingsRDDWithHeader.first()\n",
    "print(header)\n",
    "ratingsRDD = ratingsRDDWithHeader.filter(lambda x: x != header)\n",
    "ratingsRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58e03c5-603e-4a84-8326-2bba34c08cd9",
   "metadata": {},
   "source": [
    "Because RDD is unstructured, we got the full line in the file for each entry. You can simply split the string by the comma to get an array for each entry. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f1378da-2d39-45f7-bd25-8f60596f27dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '1', '4.0', '964982703'],\n",
       " ['1', '3', '4.0', '964981247'],\n",
       " ['1', '6', '4.0', '964982224'],\n",
       " ['1', '47', '5.0', '964983815'],\n",
       " ['1', '50', '5.0', '964982931']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsRDD = ratingsRDD.map(lambda x: x.split(','))\n",
    "ratingsRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43057d22-b0a0-4593-b844-2dd78359ba9b",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "Find the ID of the movie with the highest average rating. For each movie calculate the average rating and then find the maximum one. \n",
    "\n",
    "Hints: \n",
    "    <ul>\n",
    "        <li> Use the filtered RDD derived above. Use map() to generate the (movieDd,rating) tuple for all the movies in the file </li>\n",
    "        <li> Use CombineByKey to find out the ratings along with the MovieID </li>\n",
    "        <li> Use ForEach to find out the average ratings based on the movieID</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9432bce1-53c2-4e6f-966e-ba7a6492f211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131724, 5.0)\n"
     ]
    }
   ],
   "source": [
    "maprdd = ratingsRDD.map(lambda c: c.split(','))\n",
    "\n",
    "filrdd = maprdd.filter(lambda x: x[1] != \"movieID\" and x[2] != \"rating\")\n",
    "\n",
    "ratings = filrdd.map(lambda x: (int(x[1]), float(x[2])))\n",
    "combined = ratings.combineByKey(lambda c: (c, 1),\n",
    "                                lambda c, v: (c[0] + v, c[1] + 1),\n",
    "                                lambda c, v: (c[0] + v[0], c[1] + v[1]))\n",
    "\n",
    "avrg = combined.map(lambda c: (c[0], c[1][0] / c[1][1]))\n",
    "maxr = avrg.max(lambda c: c[1])\n",
    "print(f\"{maxr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ef2e4-4bb8-44f9-97d4-e880ba6f64c8",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "Load the data from the file 'movies.csv' and find the movie with the highest average rating based on the ID you found in exercise 1\n",
    "\n",
    "Hints: Use filter/sort method to find out the movie. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "35619582-e1f5-4dd8-b865-dc3ed36d5600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('131724',\n",
       "  ['131724',\n",
       "   'The Jinx: The Life and Deaths of Robert Durst (2015)',\n",
       "   'Documentary'])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masterRDDWithHeaders = sc.textFile('movies.csv')\n",
    "header = masterRDDWithHeaders.first()\n",
    "masterRDD = masterRDDWithHeaders.filter(lambda f: f != header).map(lambda x: x.split(','))\n",
    "\n",
    "movie = masterRDD.map(lambda x: (x[0], x))\n",
    "highest = movie.filter(lambda i: i[0] == '131724')\n",
    "highest.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9c2fd0-6e7f-40ec-9bbd-176df52dfb07",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Based on the result you got for the highest rated movie and based on your understanding of humans and how diverse they are in their opinion, does the average rating of this movie make sense to you?\n",
    "\n",
    "Are we done here? are you satisfied with what you've achieved and you can happily tell anyone that this is the highest average rated movie? \n",
    "\n",
    "\n",
    "If you are not, and you shouldn't be, \n",
    "1. get evidence from the data to justify why the result you achieved is not a good indicator of the highest average rated movie\n",
    "2. propose a new analysis that you think is reasonable to find the highest average rated movie \n",
    "3. find that movie based on your proposed analysis\n",
    "\n",
    "Hints: To answer this question, you should check the content of other available dataset regarding the listed movies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f634b-a373-4690-a12c-3df5adea51b5",
   "metadata": {},
   "source": [
    "Realistically, there can be a list of movies with the same average rating of 5(assuming that 5 is the highest possible rating for a movie) so it would not make sense to return only a single movie. That, and I did not take into consideration the other aspects of a movie while retrieving, say a movie with an average rating of 5 with at least 10300 reviews would make a better contender in comparison to the ones that also have a rating of 5 but only a couple of users have rated it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14efc1a-58a8-41fb-b7e8-8c2ecc4caad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
