{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac176c9a-8db8-4775-993e-75e1fd899f3f",
   "metadata": {},
   "source": [
    "# Map filter Reduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb485123-a781-4391-9fe1-d1b4f63116e5",
   "metadata": {},
   "source": [
    "MapReduce is a software framework for processing large data sets in a distributed fashion over a several machines. The core idea behind MapReduce is mapping your data set into a collection of (key, value) pairs, and then reducing over all pairs with the same key.\n",
    "\n",
    "To see the above mentioned topics in PySpark, we need to create a spark session at the beginning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf2ac8a5-bca4-4c1e-b217-7d10fb342414",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f04fc4dad10>\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "sparksession = SparkSession.builder.master(\"local[4]\") \\\n",
    "                    .appName('mapfiltershufflereduce') \\\n",
    "                    .getOrCreate()\n",
    "print(sparksession)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65650a03-489a-477a-98da-e0c782136d77",
   "metadata": {},
   "source": [
    "## Map()\n",
    "Map is a transformation function that takes a function which describes how the data elements will be transformed. Everything we are doing in Spark should work in fully distributed system! which means our trasformation of one elemenet should not be dependent on other elements in the dataset. In other words, this is an RDD transformation that is used to apply the transformation function (lambda) on every element of RDD/DataFrame and returns a new RDD. \n",
    "\n",
    "### Map() Syntax\n",
    "map(f, preservesPartitioning=False), here f is the lambda expression logic based on which the data will be mapped.\n",
    "\n",
    "Before jumping to perform Map() operation, we need to first create our RDD and over the RDD, we can use the PySpark Map() feature.\n",
    "\n",
    "### Example 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a905c2f1-3121-4b86-a82f-120e135295cc",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sparksession.sparkContext.parallelize(range(0, 10))\n",
    "print(rdd.collect())\n",
    "\n",
    "rdd.map(lambda x: x*x)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2e1bb9-9eab-4021-be28-b3d8c383bc08",
   "metadata": {},
   "source": [
    "Herem in the rdd.map() line is calling map on the RDD and the lambda function is taking the number input and returning the square of it. However when we call collect on the RDD, the values are still the same! \n",
    "\n",
    "**Remember that RDDs are immutable. Every transformation we call will return a new RDD that needs to be stored in a variable if we want to use it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade5e46b-65a5-4b47-91e4-db48c03d3c9c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squaredRDD = rdd.map(lambda x: x*x)\n",
    "squaredRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe98c175-3789-4a16-99e6-9a85ea9e8d9a",
   "metadata": {},
   "source": [
    "## Filter()\n",
    "PySpark filter() function is used to filter the rows from RDD/DataFrame based on the given condition or SQL expression, you can also use where() clause instead of the filter() if you are coming from an SQL background, both these functions operate exactly the same.\n",
    "\n",
    "### Filter() syntax\n",
    "\n",
    "filter(condition)\n",
    "\n",
    "### Example 2\n",
    "\n",
    "Let us consider, from our SquaredRDD, the lastly created, we want to drop the values which are divisable by 2. We want to keep only those values, which are non-divisible by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a37cc82-32e9-4742-ba35-d4330b3ff49d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 9, 25, 49, 81]\n"
     ]
    }
   ],
   "source": [
    "filterRDD = squaredRDD.filter(lambda x: x%2 != 0)\n",
    "print(filterRDD.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90496e3d-7d55-4d35-9543-cb22dd70f961",
   "metadata": {},
   "source": [
    "## Reduce()\n",
    "The reduce function is an action operation that will result in one output at the end. We pass to Reduce a function that takes two arguments of the same type and returns a value of the same type as well. \n",
    "\n",
    "Basically, 2 input elements will be passed to the reduce function and the return value will replace these 2 elements. And this process is repeated/parallelized until we are left with one value. This is why it is important that the 2 elements and the returned value must be of the same type.\n",
    "\n",
    "### Syntax\n",
    "reduce(f), f represents lambda expression and return\n",
    "\n",
    "### Example 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e6f6ca-685f-4e6b-bea3-744aba23d8db",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a470193c-0af0-47f4-a2d8-6d25ff29ac5b",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "For every exercise, start with a new RDD created using `parallelize` with the data you want. Take at least 15 values which are randomly generated within the range 1 to 100\n",
    "\n",
    "### Exercise 1\n",
    "\n",
    "$$\\sum _{i=0}^{10}{i^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ab3c09e-84e2-4dec-94c3-a53b866a9c61",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 22, 53, 8, 95, 42, 73, 24, 54, 40, 43, 74, 7, 12, 23]\n",
      "[4356, 484, 2809, 64, 9025, 1764, 5329, 576, 2916, 1600, 1849, 5476, 49, 144, 529]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36970"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "sparka = SparkSession.builder.master(\"local[2]\").appName(\"sparta\").getOrCreate()\n",
    "ranval = [random.randint(0, 100) for iter in range(15)]\n",
    "rdd = sparka.sparkContext.parallelize(ranval)\n",
    "print(rdd.collect())\n",
    "mapped = rdd.map(lambda x: x ** 2)\n",
    "print(mapped.collect())\n",
    "mapped.reduce(lambda x,y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da1d341-654a-4ee3-a81b-82bf1437fd8e",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "$$\\sum _{i=0}^{10}{(i+1)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f852342d-e4b4-48e4-a9c5-6df9d55aa728",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93, 28, 68, 72, 66, 60, 23, 18, 42, 8, 67, 9, 63, 95, 78]\n",
      "[8836, 841, 4761, 5329, 4489, 3721, 576, 361, 1849, 81, 4624, 100, 4096, 9216, 6241]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "55121"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "ranval = [random.randint(0, 100) for iter in range(15)]\n",
    "rdd = sparka.sparkContext.parallelize(ranval)\n",
    "print(rdd.collect())\n",
    "ordd = rdd.map(lambda x: (x + 1) ** 2)\n",
    "print(ordd.collect())\n",
    "ordd.reduce(lambda x,y: x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c51a4-cf83-4ddb-8896-b86e4cf6ebaf",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "\n",
    "$$\\prod  _{i=0}^{10}{i^2}$$\n",
    "This is production (multiply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21f1e162-5348-4fc5-b2f4-716ecc662c66",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69, 48, 45, 81, 37, 16, 41, 29, 99, 15, 17, 24, 85, 5, 38]\n",
      "[4761, 2304, 2025, 6561, 1369, 256, 1681, 841, 9801, 225, 289, 576, 7225, 25, 1444]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6913550365378912646186401233769601433600000000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "ranval = [random.randint(0, 100) for iter in range(15)]\n",
    "rdd = sparksession.sparkContext.parallelize(ranval)\n",
    "print(rdd.collect())\n",
    "ordd = rdd.map(lambda x: x ** 2)\n",
    "print(ordd.collect())\n",
    "ordd.reduce(lambda x,y: x * y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d38b9c8-e869-408a-b1f5-ac1d919948de",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "We have the following data collection. Each element has two values: x and y. \n",
    "\n",
    "- calculate the sum of all x values\n",
    "- calculate the sum of all y values\n",
    "- for each element calculate x*y and return the list of numbers\n",
    "\n",
    "ex4RDD = sc.parallelize([(1,2),(3,4),(5,6),(7,8),(9,10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9046135e-75e6-4ae2-b6d4-839ffc09f53c",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 2), (3, 4), (5, 6), (7, 8), (9, 10)]\n",
      "[1, 3, 5, 7, 9]\n",
      "25\n",
      "[2, 4, 6, 8, 10]\n",
      "30\n",
      "[2, 12, 30, 56, 90]\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "sparksession = SparkSession.builder.master(\"local[2]\").appName(\"ex-4\").getOrCreate()\n",
    "ex4RDD = sparksession.sparkContext.parallelize([(1,2),(3,4),(5,6),(7,8),(9,10)])\n",
    "print(ex4RDD.collect())\n",
    "xrdd = ex4RDD.map(lambda x: x[0])\n",
    "print(xrdd.collect())\n",
    "sx = xrdd.reduce(lambda x, y: x + y)\n",
    "print(sx)\n",
    "\n",
    "yrdd = ex4RDD.map(lambda x: x[1])\n",
    "print(yrdd.collect())\n",
    "sy = yrdd.reduce(lambda x, y: x + y)\n",
    "print(sy)\n",
    "\n",
    "mrdd = ex4RDD.map(lambda x: x[0] * x[1])\n",
    "print(mrdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185c38a-1d97-43a8-96dc-4917a8788992",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "The following data represent Houses data with the following columns:\n",
    "\"Sell\", \"List\", \"Living\", \"Rooms\", \"Beds\", \"Baths\", \"Age\", \"Acres\", \"Taxes\"\n",
    "\n",
    "Each elements is a string separating the column values by comma. \n",
    "\n",
    "Find the total amount of Taxes paid for all the houses. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5286ff5-a545-4698-baab-69503457d711",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "\"142, 160, 28, 10, 5, 3, 60, 0.28, 3167\",\n",
    "\"175, 180, 18, 8, 4, 1, 12, 0.43, 4033\",\n",
    "\"129, 132, 13, 6, 3, 1, 41, 0.33, 1471\",\n",
    "\"138, 140, 17, 7, 3, 1, 22, 0.46, 3204\",\n",
    "\"232, 240, 25, 8, 4, 3, 5, 2.05, 3613\",\n",
    "\"135, 140, 18, 7, 4, 3, 9, 0.57, 3028\",\n",
    "\"150, 160, 20, 8, 4, 3, 18, 4.00, 3131\",\n",
    "\"207, 225, 22, 8, 4, 2, 16, 2.22, 5158\",\n",
    "\"271, 285, 30, 10, 5, 2, 30, 0.53, 5702\",\n",
    "\"89,  90, 10, 5, 3, 1, 43, 0.30, 2054\",\n",
    "\"153, 157, 22, 8, 3, 3, 18, 0.38, 4127\",\n",
    "\"87,  90, 16, 7, 3, 1, 50, 0.65, 1445\",\n",
    "\"234, 238, 25, 8, 4, 2, 2, 1.61, 2087\",\n",
    "\"106, 116, 20, 8, 4, 1, 13, 0.22, 2818\",\n",
    "\"175, 180, 22, 8, 4, 2, 15, 2.06, 3917\",\n",
    "\"165, 170, 17, 8, 4, 2, 33, 0.46, 2220\",\n",
    "\"166, 170, 23, 9, 4, 2, 37, 0.27, 3498\",\n",
    "\"136, 140, 19, 7, 3, 1, 22, 0.63, 3607\",\n",
    "\"148, 160, 17, 7, 3, 2, 13, 0.36, 3648\",\n",
    "\"151, 153, 19, 8, 4, 2, 24, 0.34, 3561\",\n",
    "\"180, 190, 24, 9, 4, 2, 10, 1.55, 4681\",\n",
    "\"293, 305, 26, 8, 4, 3, 6, 0.46, 7088\",\n",
    "\"167, 170, 20, 9, 4, 2, 46, 0.46, 3482\",\n",
    "\"190, 193, 22, 9, 5, 2, 37, 0.48, 3920\",\n",
    "\"184, 190, 21, 9, 5, 2, 27, 1.30, 4162\",\n",
    "\"157, 165, 20, 8, 4, 2, 7, 0.30, 3785\",\n",
    "\"110, 115, 16, 8, 4, 1, 26, 0.29, 3103\",\n",
    "\"135, 145, 18, 7, 4, 1, 35, 0.43, 3363\",\n",
    "\"567, 625, 64, 11, 4, 4, 4, 0.85, 12192\",\n",
    "\"180, 185, 20, 8, 4, 2, 11, 1.00, 3831\",\n",
    "\"183, 188, 17, 7, 3, 2, 16, 3.00, 3564\",\n",
    "\"185, 193, 20, 9, 3, 2, 56, 6.49, 3765\",\n",
    "\"152, 155, 17, 8, 4, 1, 33, 0.70, 3361\",\n",
    "\"148, 153, 13, 6, 3, 2, 22, 0.39, 3950\",\n",
    "\"152, 159, 15, 7, 3, 1, 25, 0.59, 3055\",\n",
    "\"146, 150, 16, 7, 3, 1, 31, 0.36, 2950\",\n",
    "\"170, 190, 24, 10, 3, 2, 33, 0.57, 3346\",\n",
    "\"127, 130, 20, 8, 4, 1, 65, 0.40, 3334\",\n",
    "\"265, 270, 36, 10, 6, 3, 33, 1.20, 5853\",\n",
    "\"157, 163, 18, 8, 4, 2, 12, 1.13, 3982\",\n",
    "\"128, 135, 17, 9, 4, 1, 25, 0.52, 3374\",\n",
    "\"110, 120, 15, 8, 4, 2, 11, 0.59, 3119\",\n",
    "\"123, 130, 18, 8, 4, 2, 43, 0.39, 3268\",\n",
    "\"212, 230, 39, 12, 5, 3, 202, 4.29, 3648\",\n",
    "\"145, 145, 18, 8, 4, 2, 44, 0.22, 2783\",\n",
    "\"129, 135, 10, 6, 3, 1, 15, 1.00, 2438\",\n",
    "\"143, 145, 21, 7, 4, 2, 10, 1.20, 3529\",\n",
    "\"247, 252, 29, 9, 4, 2, 4, 1.25, 4626\",\n",
    "\"111, 120, 15, 8, 3, 1, 97, 1.11, 3205\",\n",
    "\"133, 145, 26, 7, 3, 1, 42, 0.36, 3059\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9266dac2-d0af-4d4b-8e9d-bd2cf50e5c2d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['142, 160, 28, 10, 5, 3, 60, 0.28, 3167', '175, 180, 18, 8, 4, 1, 12, 0.43, 4033', '129, 132, 13, 6, 3, 1, 41, 0.33, 1471', '138, 140, 17, 7, 3, 1, 22, 0.46, 3204', '232, 240, 25, 8, 4, 3, 5, 2.05, 3613', '135, 140, 18, 7, 4, 3, 9, 0.57, 3028', '150, 160, 20, 8, 4, 3, 18, 4.00, 3131', '207, 225, 22, 8, 4, 2, 16, 2.22, 5158', '271, 285, 30, 10, 5, 2, 30, 0.53, 5702', '89,  90, 10, 5, 3, 1, 43, 0.30, 2054', '153, 157, 22, 8, 3, 3, 18, 0.38, 4127', '87,  90, 16, 7, 3, 1, 50, 0.65, 1445', '234, 238, 25, 8, 4, 2, 2, 1.61, 2087', '106, 116, 20, 8, 4, 1, 13, 0.22, 2818', '175, 180, 22, 8, 4, 2, 15, 2.06, 3917', '165, 170, 17, 8, 4, 2, 33, 0.46, 2220', '166, 170, 23, 9, 4, 2, 37, 0.27, 3498', '136, 140, 19, 7, 3, 1, 22, 0.63, 3607', '148, 160, 17, 7, 3, 2, 13, 0.36, 3648', '151, 153, 19, 8, 4, 2, 24, 0.34, 3561', '180, 190, 24, 9, 4, 2, 10, 1.55, 4681', '293, 305, 26, 8, 4, 3, 6, 0.46, 7088', '167, 170, 20, 9, 4, 2, 46, 0.46, 3482', '190, 193, 22, 9, 5, 2, 37, 0.48, 3920', '184, 190, 21, 9, 5, 2, 27, 1.30, 4162', '157, 165, 20, 8, 4, 2, 7, 0.30, 3785', '110, 115, 16, 8, 4, 1, 26, 0.29, 3103', '135, 145, 18, 7, 4, 1, 35, 0.43, 3363', '567, 625, 64, 11, 4, 4, 4, 0.85, 12192', '180, 185, 20, 8, 4, 2, 11, 1.00, 3831', '183, 188, 17, 7, 3, 2, 16, 3.00, 3564', '185, 193, 20, 9, 3, 2, 56, 6.49, 3765', '152, 155, 17, 8, 4, 1, 33, 0.70, 3361', '148, 153, 13, 6, 3, 2, 22, 0.39, 3950', '152, 159, 15, 7, 3, 1, 25, 0.59, 3055', '146, 150, 16, 7, 3, 1, 31, 0.36, 2950', '170, 190, 24, 10, 3, 2, 33, 0.57, 3346', '127, 130, 20, 8, 4, 1, 65, 0.40, 3334', '265, 270, 36, 10, 6, 3, 33, 1.20, 5853', '157, 163, 18, 8, 4, 2, 12, 1.13, 3982', '128, 135, 17, 9, 4, 1, 25, 0.52, 3374', '110, 120, 15, 8, 4, 2, 11, 0.59, 3119', '123, 130, 18, 8, 4, 2, 43, 0.39, 3268', '212, 230, 39, 12, 5, 3, 202, 4.29, 3648', '145, 145, 18, 8, 4, 2, 44, 0.22, 2783', '129, 135, 10, 6, 3, 1, 15, 1.00, 2438', '143, 145, 21, 7, 4, 2, 10, 1.20, 3529', '247, 252, 29, 9, 4, 2, 4, 1.25, 4626', '111, 120, 15, 8, 3, 1, 97, 1.11, 3205', '133, 145, 26, 7, 3, 1, 42, 0.36, 3059']\n",
      "[3167, 4033, 1471, 3204, 3613, 3028, 3131, 5158, 5702, 2054, 4127, 1445, 2087, 2818, 3917, 2220, 3498, 3607, 3648, 3561, 4681, 7088, 3482, 3920, 4162, 3785, 3103, 3363, 12192, 3831, 3564, 3765, 3361, 3950, 3055, 2950, 3346, 3334, 5853, 3982, 3374, 3119, 3268, 3648, 2783, 2438, 3529, 4626, 3205, 3059]\n",
      "Sum of taxes: 185305\n"
     ]
    }
   ],
   "source": [
    "def getTaxes(entry):\n",
    "    values = entry.split(\",\")\n",
    "    return int(values[len(values)-1])\n",
    "\n",
    "rdd = sparksession.sparkContext.parallelize(data)\n",
    "print(rdd.collect())\n",
    "\n",
    "trdd = rdd.map(lambda x: getTaxes(x))\n",
    "print(trdd.collect())\n",
    "\n",
    "tasum = trdd.reduce(lambda x,y: x + y)\n",
    "print(f\"Sum of taxes: {tasum}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15eae5d-1c1c-4f3e-8b88-ed7321f09610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
