{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb5278b-1d80-4a0c-a0c1-f6f6dd1e2d97",
   "metadata": {},
   "source": [
    "# WordMap Counting Problem All Together\n",
    "\n",
    "We have done a lot to make our understanding better with MapReduce framework in PySpark.\n",
    "\n",
    "Now, we are going to compose all of them together .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aad7bb-fe07-436d-987a-4bdd37a6117b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf7df7c-f87b-4760-bc4b-3c0b46d61753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('word', 2),\n",
       " ('encyclopedia', 1),\n",
       " ('of', 3),\n",
       " ('words', 2),\n",
       " ('Word', 2),\n",
       " ('counting', 1),\n",
       " ('to', 2),\n",
       " ('stay', 1),\n",
       " ('certain', 1),\n",
       " ('particularly', 1),\n",
       " ('academia', 1),\n",
       " ('proceedings', 1),\n",
       " ('and', 1),\n",
       " ('used', 1),\n",
       " ('by', 1),\n",
       " ('Wikipedia', 1),\n",
       " ('the', 5),\n",
       " ('free', 1),\n",
       " ('is', 3),\n",
       " ('in', 2),\n",
       " ('document', 1),\n",
       " ('or', 1),\n",
       " ('This', 1),\n",
       " ('legal', 1),\n",
       " ('from', 1),\n",
       " ('number', 1),\n",
       " ('text', 2),\n",
       " ('needed', 1),\n",
       " ('numbers', 1),\n",
       " ('journalism', 1),\n",
       " ('for', 1),\n",
       " ('count', 3),\n",
       " ('a', 2),\n",
       " ('passage', 1),\n",
       " ('may', 2),\n",
       " ('be', 2),\n",
       " ('when', 1),\n",
       " ('required', 1),\n",
       " ('within', 1),\n",
       " ('case', 1),\n",
       " ('advertising', 1),\n",
       " ('commonly', 1),\n",
       " ('translators', 1),\n",
       " ('determine', 1),\n",
       " ('price', 1)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession.builder.master(\"local[4]\").appName(\"FlatMap-ReduceByKey\").getOrCreate();\n",
    "sc = ss.sparkContext\n",
    "\n",
    "lines = [\n",
    "    \"word count from Wikipedia the free encyclopedia\",\n",
    "    \"the word count is the number of words in a document or passage of text Word counting may be needed when a text\",\n",
    "    \"is required to stay within certain numbers of words This may particularly be the case in academia legal\",\n",
    "    \"proceedings journalism and advertising Word count is commonly used by translators to determine the price for\"\n",
    "]\n",
    "\n",
    "# Create Rdd using parallelize\n",
    "rdd = sc.parallelize(lines)\n",
    "rddFlatMap = rdd.flatMap(lambda line:line.split(\" \"))\n",
    "rddMap = rddFlatMap.map(lambda word: (word, 1))\n",
    "rddReduced = rddMap.reduceByKey(lambda x, y : x + y)\n",
    "rddReduced.collect()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f8aa4e-a1c8-40d4-b591-35898823361e",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Take the input from data.txt file and count the number of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c2eba7b-e5f6-4da7-bb40-cfb031260614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File content:\n",
      "['PySpark is a Spark library written in Python to run Python applications using Apache Spark capabilities, using PySpark we can run applications parallelly on the distributed cluster (multiple nodes). ', 'In other words, PySpark is a Python API for Apache Spark. Apache Spark is an analytical processing engine for large scale powerful distributed data processing and machine learning applications.', 'Spark basically written in Scala and later on due to its industry adaptation it’s API PySpark released for Python using Py4J. Py4J is a Java library that is integrated within PySpark and allows python to dynamically interface with JVM objects, hence to run PySpark you also need Java to be installed along with Python, and Apache Spark.']\n",
      "Applying flat map:\n",
      "['PySpark', 'is', 'a', 'Spark', 'library', 'written', 'in', 'Python', 'to', 'run', 'Python', 'applications', 'using', 'Apache', 'Spark', 'capabilities,', 'using', 'PySpark', 'we', 'can', 'run', 'applications', 'parallelly', 'on', 'the', 'distributed', 'cluster', '(multiple', 'nodes).', '', 'In', 'other', 'words,', 'PySpark', 'is', 'a', 'Python', 'API', 'for', 'Apache', 'Spark.', 'Apache', 'Spark', 'is', 'an', 'analytical', 'processing', 'engine', 'for', 'large', 'scale', 'powerful', 'distributed', 'data', 'processing', 'and', 'machine', 'learning', 'applications.', 'Spark', 'basically', 'written', 'in', 'Scala', 'and', 'later', 'on', 'due', 'to', 'its', 'industry', 'adaptation', 'it’s', 'API', 'PySpark', 'released', 'for', 'Python', 'using', 'Py4J.', 'Py4J', 'is', 'a', 'Java', 'library', 'that', 'is', 'integrated', 'within', 'PySpark', 'and', 'allows', 'python', 'to', 'dynamically', 'interface', 'with', 'JVM', 'objects,', 'hence', 'to', 'run', 'PySpark', 'you', 'also', 'need', 'Java', 'to', 'be', 'installed', 'along', 'with', 'Python,', 'and', 'Apache', 'Spark.']\n",
      "Word count: 116\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ses = SparkSession.builder.master(\"local[2]\").appName(\"mapreduce\").getOrCreate()\n",
    "\n",
    "print(\"File content:\")\n",
    "rdd = ses.sparkContext.textFile(\"data.txt\")\n",
    "print(rdd.collect())\n",
    "\n",
    "print(\"Applying flat map:\")\n",
    "flatrdd = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "print(flatrdd.collect())\n",
    "\n",
    "wc = len(flatrdd.collect())\n",
    "print(\"Word count: \" + str(wc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cec6ce3-f131-4f40-94d7-08a8b63a18ec",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "Load the testData.txt in the rdd and add all the values together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0123ed1-f39f-4b7e-9d0b-0fdf33ef7911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12 34 56 77 89 23 12 34 56 77 89 23 12 34 56 77 89 23 12 34 56 77 89 23 12 34 56 77 89 23 12 34 56 77 89 23']\n",
      "['12', '34', '56', '77', '89', '23', '12', '34', '56', '77', '89', '23', '12', '34', '56', '77', '89', '23', '12', '34', '56', '77', '89', '23', '12', '34', '56', '77', '89', '23', '12', '34', '56', '77', '89', '23']\n",
      "1746\n"
     ]
    }
   ],
   "source": [
    "rdd = ses.sparkContext.textFile(\"testData.txt\")\n",
    "print(rdd.collect())\n",
    "ordd = rdd.flatMap(lambda x: x.split(\" \"))\n",
    "print(ordd.collect())\n",
    "total = ordd.reduce(lambda x, y: int(x) + int(y))\n",
    "\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3c770b-4517-41f6-ac00-ba04ecf97dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
